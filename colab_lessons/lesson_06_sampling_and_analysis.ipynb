{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63122dd0",
   "metadata": {},
   "source": [
    "# 音のプログラミング 第6回: サンプリング・音声分析・リアルタイム処理\n",
    "\n",
    "**学習目標:**\n",
    "- サンプリング理論の基礎を理解する\n",
    "- 音声ファイルの読み込みと分析を行う\n",
    "- フーリエ変換による周波数分析を体験する\n",
    "- スペクトログラムで音の可視化を学ぶ\n",
    "- リアルタイム音声処理の基礎を体験する\n",
    "\n",
    "**所要時間:** 90分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cbf238",
   "metadata": {},
   "source": [
    "## 🛠️ 環境設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13047128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab環境の確認とセットアップ\n",
    "import sys\n",
    "\n",
    "# Colab環境かどうかを確認\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"🔧 Google Colab環境を設定中...\")\n",
    "    \n",
    "    # 必要なパッケージをインストール\n",
    "    !pip install numpy scipy matplotlib ipython librosa\n",
    "    \n",
    "    # GitHubからライブラリをクローン\n",
    "    !git clone https://github.com/ggszk/simple-audio-programming.git\n",
    "    \n",
    "    # パスを追加\n",
    "    sys.path.append('/content/simple-audio-programming')\n",
    "    \n",
    "    print(\"✅ セットアップ完了！\")\n",
    "    print(\"📝 このノートブックを自分用にコピーするには:\")\n",
    "    print(\"   ファイル → ドライブにコピーを保存\")\n",
    "    \n",
    "else:\n",
    "    print(\"🏠 ローカル環境で実行中\")\n",
    "    print(\"📝 audio_libがインストールされていることを確認してください\")\n",
    "\n",
    "# インポート文\n",
    "from audio_lib.core import AudioConfig, load_audio, save_audio\n",
    "from audio_lib.synthesis import SineWave\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "config = AudioConfig()\n",
    "\n",
    "print(\"\\n🎵 サンプリングと音響分析の学習を始めましょう！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f9a15c",
   "metadata": {},
   "source": [
    "## 🔬 サンプリング理論の基礎\n",
    "\n",
    "### デジタル音声の3つの基本要素\n",
    "1. **サンプリング周波数 (Sample Rate)**: 1秒間に何回測定するか\n",
    "2. **ビット深度 (Bit Depth)**: 各測定値をどれだけ細かく記録するか\n",
    "3. **チャンネル数**: モノラル(1)かステレオ(2)か\n",
    "\n",
    "### ナイキスト定理\n",
    "**元の信号を正確に復元するには、最高周波数の2倍以上でサンプリングする必要がある**\n",
    "\n",
    "- 人間の可聴域: 20Hz ～ 20kHz\n",
    "- CD品質: 44.1kHz サンプリング（20kHz × 2 + 余裕）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b4a12",
   "metadata": {},
   "source": [
    "## 🎯 実習1: サンプリング周波数の影響を体験しよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5cc00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 元の信号（高品質）\n",
    "duration = 2.0\n",
    "frequency = 1000  # 1kHz\n",
    "\n",
    "# 高サンプリング周波数（44.1kHz）\n",
    "high_sr = 44100\n",
    "t_high = np.linspace(0, duration, int(high_sr * duration), False)\n",
    "signal_high = np.sin(2 * np.pi * frequency * t_high)\n",
    "\n",
    "# 低サンプリング周波数（8kHz）\n",
    "low_sr = 8000\n",
    "t_low = np.linspace(0, duration, int(low_sr * duration), False)\n",
    "signal_low = np.sin(2 * np.pi * frequency * t_low)\n",
    "\n",
    "# 超低サンプリング周波数（1.5kHz - ナイキスト定理違反）\n",
    "very_low_sr = 1500\n",
    "t_very_low = np.linspace(0, duration, int(very_low_sr * duration), False)\n",
    "signal_very_low = np.sin(2 * np.pi * frequency * t_very_low)\n",
    "\n",
    "print(\"🔊 高品質 (44.1kHz)\")\n",
    "display(Audio(signal_high, rate=high_sr))\n",
    "\n",
    "print(\"🔊 中品質 (8kHz)\")\n",
    "display(Audio(signal_low, rate=low_sr))\n",
    "\n",
    "print(\"🚨 低品質 (1.5kHz) - エイリアシング発生\")\n",
    "display(Audio(signal_very_low, rate=very_low_sr))\n",
    "\n",
    "print(\"\\n💡 サンプリング周波数が低すぎると、元の音と全く違う音になります！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c098564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 波形の可視化\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 最初の0.01秒だけ表示（詳細を見るため）\n",
    "zoom_duration = 0.01\n",
    "zoom_samples_high = int(high_sr * zoom_duration)\n",
    "zoom_samples_low = int(low_sr * zoom_duration)\n",
    "zoom_samples_very_low = int(very_low_sr * zoom_duration)\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(t_high[:zoom_samples_high], signal_high[:zoom_samples_high], 'b-', linewidth=2, label='44.1kHz')\n",
    "plt.title('高サンプリング周波数 (44.1kHz)', fontsize=14)\n",
    "plt.ylabel('振幅')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(t_low[:zoom_samples_low], signal_low[:zoom_samples_low], 'g-o', linewidth=2, markersize=4, label='8kHz')\n",
    "plt.title('中サンプリング周波数 (8kHz)', fontsize=14)\n",
    "plt.ylabel('振幅')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(t_very_low[:zoom_samples_very_low], signal_very_low[:zoom_samples_very_low], 'r-o', linewidth=2, markersize=6, label='1.5kHz')\n",
    "plt.title('低サンプリング周波数 (1.5kHz) - エイリアシング', fontsize=14)\n",
    "plt.xlabel('時間 (秒)')\n",
    "plt.ylabel('振幅')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"📊 点の密度でサンプリング周波数の違いがわかります\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611a712c",
   "metadata": {},
   "source": [
    "## 🎵 音声ファイルの作成と分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37326ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 複雑な音を作成（和音）\n",
    "duration = 3.0\n",
    "sample_rate = config.sample_rate\n",
    "\n",
    "# Cメジャーコード（ド・ミ・ソ）\n",
    "c_major_freqs = [261.63, 329.63, 392.00]  # C4, E4, G4\n",
    "\n",
    "# 各音を生成\n",
    "chord_signal = np.zeros(int(sample_rate * duration))\n",
    "adsr = ADSREnvelope(attack=0.1, decay=0.2, sustain=0.7, release=0.5, config=config)\n",
    "envelope_data = adsr.generate(duration)\n",
    "\n",
    "for freq in c_major_freqs:\n",
    "    tone = sine_osc.generate(freq, duration)\n",
    "    tone_with_env = apply_envelope(tone, envelope_data)\n",
    "    chord_signal += tone_with_env * 0.3  # 音量調整\n",
    "\n",
    "# WAVファイルとして保存\n",
    "sf.write('c_major_chord.wav', chord_signal, sample_rate)\n",
    "\n",
    "print(\"🎵 Cメジャーコードを作成しました\")\n",
    "display(Audio(chord_signal, rate=sample_rate))\n",
    "print(\"📁 'c_major_chord.wav' として保存されました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a298afd",
   "metadata": {},
   "source": [
    "## 🎯 実習2: 音声ファイルの読み込みと基本分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b5928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 音声ファイルの読み込み\n",
    "audio_data, sr = librosa.load('c_major_chord.wav', sr=None)\n",
    "\n",
    "print(f\"📊 音声ファイル情報:\")\n",
    "print(f\"   サンプリング周波数: {sr} Hz\")\n",
    "print(f\"   データ長: {len(audio_data)} サンプル\")\n",
    "print(f\"   再生時間: {len(audio_data) / sr:.2f} 秒\")\n",
    "print(f\"   最大振幅: {np.max(np.abs(audio_data)):.3f}\")\n",
    "print(f\"   RMS（実効値）: {np.sqrt(np.mean(audio_data**2)):.3f}\")\n",
    "\n",
    "# 波形の可視化\n",
    "time_axis = np.linspace(0, len(audio_data) / sr, len(audio_data))\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(time_axis, audio_data, 'b-', linewidth=1)\n",
    "plt.title('Cメジャーコード - 時間波形', fontsize=16)\n",
    "plt.xlabel('時間 (秒)')\n",
    "plt.ylabel('振幅')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 最初の0.1秒の拡大表示\n",
    "zoom_samples = int(0.1 * sr)\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(time_axis[:zoom_samples], audio_data[:zoom_samples], 'r-', linewidth=2)\n",
    "plt.title('最初の0.1秒（拡大）', fontsize=16)\n",
    "plt.xlabel('時間 (秒)')\n",
    "plt.ylabel('振幅')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f480af",
   "metadata": {},
   "source": [
    "## 🌈 フーリエ変換による周波数分析\n",
    "\n",
    "### フーリエ変換とは？\n",
    "**時間領域の信号を周波数領域に変換する数学的技法**\n",
    "\n",
    "- 時間波形 → どの周波数成分がどれだけ含まれているか\n",
    "- 和音の各音程を分離して見ることができる\n",
    "- 楽器の音色分析に使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b65e5",
   "metadata": {},
   "source": [
    "## 🎯 実習3: FFT（高速フーリエ変換）による周波数分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86216cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFTの実行\n",
    "# 中央部分（安定した部分）を抽出して分析\n",
    "start_sample = int(0.5 * sr)  # 0.5秒後から\n",
    "window_length = int(0.5 * sr)  # 0.5秒間\n",
    "analysis_window = audio_data[start_sample:start_sample + window_length]\n",
    "\n",
    "# 窓関数を適用（スペクトル漏れを防ぐ）\n",
    "windowed_signal = analysis_window * np.hanning(len(analysis_window))\n",
    "\n",
    "# FFT実行\n",
    "fft_result = np.fft.fft(windowed_signal)\n",
    "fft_magnitude = np.abs(fft_result)\n",
    "fft_freq = np.fft.fftfreq(len(windowed_signal), 1/sr)\n",
    "\n",
    "# 正の周波数のみ（対称性を利用）\n",
    "positive_freq_idx = fft_freq >= 0\n",
    "freq_positive = fft_freq[positive_freq_idx]\n",
    "magnitude_positive = fft_magnitude[positive_freq_idx]\n",
    "\n",
    "# デシベル変換\n",
    "magnitude_db = 20 * np.log10(magnitude_positive + 1e-10)  # ゼロ除算防止\n",
    "\n",
    "# 結果の可視化\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(freq_positive[:len(freq_positive)//10], magnitude_positive[:len(magnitude_positive)//10], 'g-', linewidth=2)\n",
    "plt.title('周波数スペクトラム（線形スケール）', fontsize=16)\n",
    "plt.xlabel('周波数 (Hz)')\n",
    "plt.ylabel('振幅')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 期待される周波数にマーカーを追加\n",
    "for freq, note in zip(c_major_freqs, ['C4', 'E4', 'G4']):\n",
    "    plt.axvline(x=freq, color='red', linestyle='--', alpha=0.7, label=f'{note} ({freq:.1f} Hz)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(freq_positive[:len(freq_positive)//10], magnitude_db[:len(magnitude_db)//10], 'b-', linewidth=2)\n",
    "plt.title('周波数スペクトラム（デシベルスケール）', fontsize=16)\n",
    "plt.xlabel('周波数 (Hz)')\n",
    "plt.ylabel('振幅 (dB)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 期待される周波数にマーカーを追加\n",
    "for freq, note in zip(c_major_freqs, ['C4', 'E4', 'G4']):\n",
    "    plt.axvline(x=freq, color='red', linestyle='--', alpha=0.7, label=f'{note} ({freq:.1f} Hz)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"🎵 和音の各音程が周波数スペクトラムに現れています！\")\n",
    "print(\"赤い破線が理論値、実際のピークと比較してみてください。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d49d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ピーク検出\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# 2000Hz以下の低周波域でピークを検出\n",
    "low_freq_idx = freq_positive < 2000\n",
    "low_freq = freq_positive[low_freq_idx]\n",
    "low_magnitude = magnitude_positive[low_freq_idx]\n",
    "\n",
    "# ピーク検出（閾値と最小距離を設定）\n",
    "peaks, properties = find_peaks(low_magnitude, height=np.max(low_magnitude)*0.1, distance=20)\n",
    "\n",
    "print(\"🔍 検出されたピーク周波数:\")\n",
    "for i, peak_idx in enumerate(peaks):\n",
    "    detected_freq = low_freq[peak_idx]\n",
    "    magnitude = low_magnitude[peak_idx]\n",
    "    \n",
    "    # 最も近い理論値を見つける\n",
    "    closest_theoretical = min(c_major_freqs, key=lambda x: abs(x - detected_freq))\n",
    "    error = abs(detected_freq - closest_theoretical)\n",
    "    \n",
    "    print(f\"   ピーク {i+1}: {detected_freq:.1f} Hz (理論値: {closest_theoretical:.1f} Hz, 誤差: {error:.1f} Hz)\")\n",
    "\n",
    "print(\"\\n💡 FFTで和音の構成音を正確に分析できました！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6241b3b",
   "metadata": {},
   "source": [
    "## 🎯 実習4: スペクトログラム（時間-周波数分析）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93899ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 時間変化する音を作成\n",
    "duration = 4.0\n",
    "changing_signal = np.array([])\n",
    "\n",
    "# スケール演奏（ド・レ・ミ・ファ・ソ・ラ・シ・ド）\n",
    "scale_freqs = [261.63, 293.66, 329.63, 349.23, 392.00, 440.00, 493.88, 523.25]\n",
    "note_duration = duration / len(scale_freqs)\n",
    "\n",
    "adsr_short = ADSREnvelope(attack=0.05, decay=0.1, sustain=0.8, release=0.15, config=config)\n",
    "\n",
    "for freq in scale_freqs:\n",
    "    note = sine_osc.generate(freq, note_duration)\n",
    "    envelope = adsr_short.generate(note_duration)\n",
    "    note_with_env = apply_envelope(note, envelope)\n",
    "    changing_signal = np.concatenate([changing_signal, note_with_env])\n",
    "\n",
    "print(\"🎵 音階演奏\")\n",
    "display(Audio(changing_signal, rate=sample_rate))\n",
    "\n",
    "# スペクトログラムの計算\n",
    "frequencies, times, Sxx = signal.spectrogram(\n",
    "    changing_signal, \n",
    "    fs=sample_rate,\n",
    "    window='hann',\n",
    "    nperseg=1024,\n",
    "    noverlap=512\n",
    ")\n",
    "\n",
    "# スペクトログラムの可視化\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "time_axis = np.linspace(0, len(changing_signal) / sample_rate, len(changing_signal))\n",
    "plt.plot(time_axis, changing_signal, 'b-', linewidth=1)\n",
    "plt.title('時間波形（音階演奏）', fontsize=16)\n",
    "plt.xlabel('時間 (秒)')\n",
    "plt.ylabel('振幅')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.pcolormesh(times, frequencies[:200], 10*np.log10(Sxx[:200] + 1e-10), shading='gouraud', cmap='viridis')\n",
    "plt.title('スペクトログラム（時間-周波数分析）', fontsize=16)\n",
    "plt.xlabel('時間 (秒)')\n",
    "plt.ylabel('周波数 (Hz)')\n",
    "plt.colorbar(label='パワー (dB)')\n",
    "\n",
    "# 理論的な音程を水平線で表示\n",
    "for freq, note in zip(scale_freqs, ['C4', 'D4', 'E4', 'F4', 'G4', 'A4', 'B4', 'C5']):\n",
    "    plt.axhline(y=freq, color='white', linestyle='--', alpha=0.7, linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"🌈 スペクトログラムで音程の時間変化が見えます！\")\n",
    "print(\"明るい色ほど強い周波数成分を表します。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fdd38d",
   "metadata": {},
   "source": [
    "## 🎯 実習5: リアルタイム音声処理の模擬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad9838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# バッファサイズ（リアルタイム処理のブロックサイズ）\n",
    "buffer_size = 1024\n",
    "overlap = buffer_size // 2\n",
    "\n",
    "# 処理対象の音声（前回作成した音階）\n",
    "input_signal = changing_signal\n",
    "output_signal = np.zeros_like(input_signal)\n",
    "\n",
    "# シンプルなローパスフィルターの設計\n",
    "lpf = LowPassFilter(cutoff_freq=1000, config=config)\n",
    "\n",
    "print(\"🔄 リアルタイム処理シミュレーション開始\")\n",
    "print(f\"バッファサイズ: {buffer_size} サンプル ({buffer_size/sample_rate*1000:.1f} ms)\")\n",
    "\n",
    "# バッファごとの処理\n",
    "processing_times = []\n",
    "num_buffers = (len(input_signal) - overlap) // (buffer_size - overlap)\n",
    "\n",
    "for i in range(num_buffers):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # バッファの開始・終了位置\n",
    "    start_idx = i * (buffer_size - overlap)\n",
    "    end_idx = start_idx + buffer_size\n",
    "    \n",
    "    if end_idx > len(input_signal):\n",
    "        break\n",
    "    \n",
    "    # 入力バッファ\n",
    "    input_buffer = input_signal[start_idx:end_idx]\n",
    "    \n",
    "    # 処理（ローパスフィルター適用）\n",
    "    filtered_buffer = lpf.process(input_buffer)\n",
    "    \n",
    "    # 出力バッファに書き込み（オーバーラップ加算）\n",
    "    output_end_idx = min(start_idx + buffer_size, len(output_signal))\n",
    "    output_length = output_end_idx - start_idx\n",
    "    output_signal[start_idx:output_end_idx] += filtered_buffer[:output_length]\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    processing_times.append(processing_time)\n",
    "    \n",
    "    if i % 10 == 0:  # 10バッファごとに進捗表示\n",
    "        print(f\"   バッファ {i+1}/{num_buffers} 処理完了 ({processing_time*1000:.2f} ms)\")\n",
    "\n",
    "print(\"\\n✅ 処理完了\")\n",
    "print(f\"平均処理時間: {np.mean(processing_times)*1000:.2f} ms/buffer\")\n",
    "print(f\"最大処理時間: {np.max(processing_times)*1000:.2f} ms/buffer\")\n",
    "print(f\"リアルタイム余裕度: {(buffer_size/sample_rate) / np.mean(processing_times):.1f}x\")\n",
    "\n",
    "print(\"\\n🔊 オリジナル音声\")\n",
    "display(Audio(input_signal, rate=sample_rate))\n",
    "\n",
    "print(\"\\n🔊 フィルター適用後\")\n",
    "display(Audio(output_signal, rate=sample_rate))\n",
    "\n",
    "print(\"\\n💡 高音成分がカットされ、柔らかい音になりました！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d744b753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 処理前後の比較（スペクトラム）\n",
    "# 中央部分を分析\n",
    "analysis_start = len(input_signal) // 4\n",
    "analysis_length = len(input_signal) // 2\n",
    "analysis_end = analysis_start + analysis_length\n",
    "\n",
    "input_segment = input_signal[analysis_start:analysis_end]\n",
    "output_segment = output_signal[analysis_start:analysis_end]\n",
    "\n",
    "# FFT\n",
    "input_fft = np.fft.fft(input_segment * np.hanning(len(input_segment)))\n",
    "output_fft = np.fft.fft(output_segment * np.hanning(len(output_segment)))\n",
    "\n",
    "freqs = np.fft.fftfreq(len(input_segment), 1/sample_rate)\n",
    "positive_idx = freqs >= 0\n",
    "freqs_pos = freqs[positive_idx]\n",
    "input_magnitude = np.abs(input_fft[positive_idx])\n",
    "output_magnitude = np.abs(output_fft[positive_idx])\n",
    "\n",
    "# 比較グラフ\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(freqs_pos[:len(freqs_pos)//20], 20*np.log10(input_magnitude[:len(input_magnitude)//20] + 1e-10), \n",
    "         'b-', linewidth=2, label='処理前')\n",
    "plt.plot(freqs_pos[:len(freqs_pos)//20], 20*np.log10(output_magnitude[:len(output_magnitude)//20] + 1e-10), \n",
    "         'r-', linewidth=2, label='処理後（LPF適用）')\n",
    "plt.axvline(x=1000, color='green', linestyle='--', alpha=0.7, label='カットオフ周波数 (1kHz)')\n",
    "plt.title('ローパスフィルター効果の周波数特性', fontsize=16)\n",
    "plt.xlabel('周波数 (Hz)')\n",
    "plt.ylabel('振幅 (dB)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "time_axis = np.linspace(0, len(input_signal) / sample_rate, len(input_signal))\n",
    "plt.plot(time_axis, input_signal, 'b-', alpha=0.7, linewidth=1, label='処理前')\n",
    "plt.plot(time_axis, output_signal, 'r-', alpha=0.7, linewidth=1, label='処理後')\n",
    "plt.title('時間波形の比較', fontsize=16)\n",
    "plt.xlabel('時間 (秒)')\n",
    "plt.ylabel('振幅')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"📊 1kHz以上の高周波成分が除去されています\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6e612b",
   "metadata": {},
   "source": [
    "## 🏆 チャレンジ課題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f312d4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# チャレンジ1: あなただけの音を分析しよう\n",
    "# 複数のオシレーターを組み合わせて複雑な音を作り、分析してください\n",
    "\n",
    "from audio_lib import SquareWave, SawtoothWave\n",
    "\n",
    "# 異なる波形のオシレーター\n",
    "square_osc = SquareWave(config)\n",
    "saw_osc = SawtoothWave(config)\n",
    "\n",
    "# あなたのオリジナル音（パラメータを変更してください）\n",
    "duration = 2.0\n",
    "fundamental_freq = 220  # A3\n",
    "\n",
    "# 複数の波形を組み合わせ\n",
    "sine_component = sine_osc.generate(fundamental_freq, duration) * 0.5\n",
    "square_component = square_osc.generate(fundamental_freq, duration) * 0.3\n",
    "saw_component = saw_osc.generate(fundamental_freq * 1.5, duration) * 0.2  # 5度上\n",
    "\n",
    "# 合成\n",
    "complex_signal = sine_component + square_component + saw_component\n",
    "\n",
    "# エンベロープ適用\n",
    "envelope = ADSREnvelope(attack=0.1, decay=0.3, sustain=0.6, release=0.8, config=config)\n",
    "env_data = envelope.generate(duration)\n",
    "complex_signal = apply_envelope(complex_signal, env_data)\n",
    "\n",
    "print(\"🎵 あなたのオリジナル音色\")\n",
    "display(Audio(complex_signal, rate=sample_rate))\n",
    "\n",
    "# 分析\n",
    "complex_fft = np.fft.fft(complex_signal * np.hanning(len(complex_signal)))\n",
    "complex_freqs = np.fft.fftfreq(len(complex_signal), 1/sample_rate)\n",
    "positive_idx = complex_freqs >= 0\n",
    "complex_freqs_pos = complex_freqs[positive_idx]\n",
    "complex_magnitude = np.abs(complex_fft[positive_idx])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(complex_freqs_pos[:len(complex_freqs_pos)//10], \n",
    "         20*np.log10(complex_magnitude[:len(complex_magnitude)//10] + 1e-10), \n",
    "         'purple', linewidth=2)\n",
    "plt.axvline(x=fundamental_freq, color='red', linestyle='--', alpha=0.7, label=f'基音 ({fundamental_freq} Hz)')\n",
    "plt.axvline(x=fundamental_freq*1.5, color='orange', linestyle='--', alpha=0.7, label=f'5度 ({fundamental_freq*1.5} Hz)')\n",
    "plt.title('あなたのオリジナル音色 - 周波数分析', fontsize=16)\n",
    "plt.xlabel('周波数 (Hz)')\n",
    "plt.ylabel('振幅 (dB)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"🔍 倍音成分や組み合わせた波形の特徴が見えますか？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1ee426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# チャレンジ2: エフェクト付きリアルタイム処理\n",
    "# 複数のエフェクトを組み合わせてリアルタイム処理してみましょう\n",
    "\n",
    "# 入力信号（あなたのオリジナル音）\n",
    "input_signal = complex_signal\n",
    "output_signal = np.zeros_like(input_signal)\n",
    "\n",
    "# エフェクトチェーン\n",
    "lpf = LowPassFilter(cutoff_freq=2000, config=config)\n",
    "# 必要に応じて他のエフェクトも追加できます\n",
    "\n",
    "# バッファ処理\n",
    "buffer_size = 512\n",
    "num_buffers = len(input_signal) // buffer_size\n",
    "\n",
    "print(\"🎛️ エフェクトチェーン処理開始\")\n",
    "\n",
    "for i in range(num_buffers):\n",
    "    start_idx = i * buffer_size\n",
    "    end_idx = start_idx + buffer_size\n",
    "    \n",
    "    if end_idx > len(input_signal):\n",
    "        break\n",
    "    \n",
    "    # 入力バッファ\n",
    "    input_buffer = input_signal[start_idx:end_idx]\n",
    "    \n",
    "    # エフェクト適用\n",
    "    processed_buffer = lpf.process(input_buffer)\n",
    "    # 必要に応じてさらなるエフェクトを追加\n",
    "    \n",
    "    # 出力\n",
    "    output_signal[start_idx:end_idx] = processed_buffer\n",
    "\n",
    "print(\"✅ エフェクト処理完了\")\n",
    "\n",
    "print(\"\\n🔊 エフェクト前\")\n",
    "display(Audio(input_signal, rate=sample_rate))\n",
    "\n",
    "print(\"\\n🔊 エフェクト後\")\n",
    "display(Audio(output_signal, rate=sample_rate))\n",
    "\n",
    "print(\"\\n🎉 あなただけのエフェクト音の完成です！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a06bc9e",
   "metadata": {},
   "source": [
    "## 📚 今日のまとめ\n",
    "\n",
    "### 学んだこと\n",
    "1. **サンプリング理論**: ナイキスト定理とエイリアシング\n",
    "2. **音声ファイル処理**: 読み込み、基本情報の取得\n",
    "3. **フーリエ変換**: 時間領域→周波数領域の変換\n",
    "4. **周波数分析**: FFTによるスペクトラム解析\n",
    "5. **スペクトログラム**: 時間-周波数の2次元分析\n",
    "6. **リアルタイム処理**: バッファベースの処理技法\n",
    "\n",
    "### 使ったライブラリ・技術\n",
    "- `numpy.fft`: 高速フーリエ変換\n",
    "- `scipy.signal`: スペクトログラム、ピーク検出\n",
    "- `librosa`: 音声ファイル読み込み\n",
    "- `soundfile`: 音声ファイル書き込み\n",
    "- 窓関数、オーバーラップ処理\n",
    "\n",
    "### 実用的な応用\n",
    "- **音楽分析**: 楽器認識、音程検出\n",
    "- **音響効果**: リアルタイムエフェクト\n",
    "- **音声処理**: ノイズ除去、音質改善\n",
    "- **研究開発**: 音響特性の解析\n",
    "\n",
    "### デジタル音声の基本\n",
    "- サンプリング周波数とビット深度の重要性\n",
    "- 周波数領域での音の理解\n",
    "- リアルタイム制約と処理効率\n",
    "\n",
    "### 次回予告\n",
    "いよいよ最終回！「**音楽制作プロジェクト・発表会**」\n",
    "これまでの学習を総動員して、あなただけのオリジナル音楽作品を作ります。\n",
    "楽器の音色設計から楽曲構成まで、クリエイティブな音楽制作に挑戦しましょう！\n",
    "\n",
    "---\n",
    "**お疲れさまでした！** 🎉\n",
    "\n",
    "音の世界の奥深さを感じていただけたでしょうか？\n",
    "次回は、これまで学んだ全てを使って素晴らしい作品を作りましょう！"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
